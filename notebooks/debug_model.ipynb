{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../tensorflow_fcos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.initializers import RandomNormal, Constant\n",
    "from tensorflow.keras.layers import (Input,\n",
    "                                     Reshape,\n",
    "                                     ReLU,\n",
    "                                     Add)\n",
    "from models.blocks import conv_block, upsample_like\n",
    "from models.custom_layers import Scale\n",
    "from pprint import pprint\n",
    "from data.bdd_dataset.dataset import dataset_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCOS:\n",
    "    def __init__(self, config):\n",
    "        self._validate_config(config)\n",
    "        for attr in config:\n",
    "            setattr(self, attr, config[attr])\n",
    "        self._build_fpn()\n",
    "        self._build_model()\n",
    "        self._build_datasets()\n",
    "        self._build_optimizer()\n",
    "\n",
    "    def _validate_config(self, config):\n",
    "        attr_list = [\n",
    "            'mode',\n",
    "            'distribute_strategy',\n",
    "            'image_height',\n",
    "            'image_width',\n",
    "            'num_classes',\n",
    "            'data_dir',\n",
    "            'dataset_fn',\n",
    "            'batch_size',\n",
    "            'epochs',\n",
    "            'learning_rate',\n",
    "            'model_dir',\n",
    "            'tensorboard_log_dir',\n",
    "            'restore_parameters'\n",
    "        ]\n",
    "        for attr in attr_list:\n",
    "            assert attr in config, 'Missing {} in config'.format(attr)\n",
    "        pprint('****Initializing FCOS with the following config')\n",
    "        pprint(config)\n",
    "\n",
    "    def _build_fpn(self):\n",
    "        '''\n",
    "            From the FPN paper, \"To start the iteration, we simply attach a\n",
    "            1×1 convolutional layer on C5 to produce the coarsest resolution\n",
    "            map. Finally, we append a 3×3 convolution on each merged map to\n",
    "            generate the final feature map, which is to reduce the aliasing\n",
    "            effect of upsampling. This final set of feature maps is called\n",
    "            {P2, P3, P4, P5}, corresponding to {C2, C3, C4, C5} that are\n",
    "            respectively of the same spatial sizes\".\n",
    "            From the FCOS paper, \"P6 and P7 are produced by applying one\n",
    "            convolutional layer with the stride being 2 on P5 and P6,\n",
    "            respectively\".\n",
    "        '''\n",
    "        with self.distribute_strategy.scope():\n",
    "            pprint('****Building FPN')\n",
    "            self._backbone = tf.keras.applications.ResNet50V2(\n",
    "                input_shape=[self.image_height, self.image_width, 3],\n",
    "                weights='imagenet',\n",
    "                include_top=False)\n",
    "            C5 = self._backbone.get_layer('post_relu').output\n",
    "            C4 = self._backbone.get_layer('conv4_block6_1_relu').output\n",
    "            C3 = self._backbone.get_layer('conv3_block4_1_relu').output\n",
    "\n",
    "            M5 = conv_block(C5, 256, 1, bn_act=False, name_prefix='C5')\n",
    "            P5 = conv_block(M5, 256, 3, bn_act=False, name_prefix='P5')\n",
    "            M5_upsampled = upsample_like(M5, C4, name='M5_upsampled')\n",
    "\n",
    "            M4 = conv_block(C4, 256, 1, bn_act=False, name_prefix='C4')\n",
    "            M4 = tf.keras.layers.Add(name='M4_M5_add')([M4, M5_upsampled])\n",
    "            P4 = conv_block(M4, 256, 3, bn_act=False, name_prefix='P4')\n",
    "            M4_upsampled = upsample_like(M4, C3, name='M4_upsampled')\n",
    "\n",
    "            M3 = conv_block(C3, 256, 1, bn_act=False, name_prefix='C3')\n",
    "            P3 = Add(name='M3_M4_add')([M3, M4_upsampled])\n",
    "            P3 = conv_block(P3, 256, 3, bn_act=False, name_prefix='P3')\n",
    "\n",
    "            P6 = conv_block(P5, 256, 3, 2, bn_act=False, name_prefix='P6')\n",
    "            P6_relu = ReLU(name='P6_relu')(P6)\n",
    "            P7 = conv_block(P6_relu, 256, 3, 2, bn_act=False, name_prefix='P7')\n",
    "\n",
    "            self._pyramid_features = {\n",
    "                'P3': P3,\n",
    "                'P4': P4,\n",
    "                'P5': P5,\n",
    "                'P6': P6,\n",
    "                'P7': P7\n",
    "            }\n",
    "\n",
    "    def _get_classification_head(self, p=0.01):\n",
    "        kernel_init = RandomNormal(0.0, 0.01)\n",
    "        bias_init = Constant(-np.log((1 - p) / p))\n",
    "\n",
    "        input_layer = Input(shape=[None, None, 256])\n",
    "        x = input_layer\n",
    "\n",
    "        for i in range(4):\n",
    "            x = conv_block(x, 256, 3, kernel_init=kernel_init,\n",
    "                           bn_act=False, name_prefix='c_head_{}'.format(i))\n",
    "        classification_logits = conv_block(x, self.num_classes,\n",
    "                                           3, kernel_init=kernel_init,\n",
    "                                           bias_init=bias_init, bn_act=False,\n",
    "                                           name_prefix='cls_logits')\n",
    "        centerness_logits = conv_block(x, 1, 3,\n",
    "                                       kernel_init=kernel_init, bn_act=False,\n",
    "                                       name_prefix='ctr_logits')\n",
    "        classification_logits = Reshape(\n",
    "            target_shape=[-1, self.num_classes])(classification_logits)\n",
    "        centerness_logits = Reshape(target_shape=[-1, 1])(centerness_logits)\n",
    "\n",
    "        outputs = [classification_logits, centerness_logits]\n",
    "        return tf.keras.Model(inputs=[input_layer],\n",
    "                              outputs=[outputs],\n",
    "                              name='classification_head')\n",
    "\n",
    "    def _get_regression_head(self):\n",
    "        kernel_init = RandomNormal(0.0, 0.01)\n",
    "        input_layer = Input(shape=[None, None, 256])\n",
    "        x = input_layer\n",
    "\n",
    "        for i in range(4):\n",
    "            x = conv_block(x, 256, 3, kernel_init=kernel_init,\n",
    "                           bn_act=False, name_prefix='r_head_{}'.format(i))\n",
    "        regression_logits = conv_block(x, 4, 3, kernel_init=kernel_init,\n",
    "                                       bn_act=False, name_prefix='reg_logits')\n",
    "        regression_logits = Reshape(target_shape=[-1, 4])(regression_logits)\n",
    "        return tf.keras.Model(inputs=[input_layer],\n",
    "                              outputs=[regression_logits],\n",
    "                              name='regression_head')\n",
    "\n",
    "    def _build_model(self):\n",
    "        with self.distribute_strategy.scope():\n",
    "            pprint('****Building FCOS')\n",
    "            self._classification_head = self._get_classification_head()\n",
    "            self._regression_head = self._get_regression_head()\n",
    "\n",
    "            _classification_logits = []\n",
    "            _centerness_logits = []\n",
    "            _regression_logits = []\n",
    "\n",
    "            for i in range(3, 8):\n",
    "                feature = self._pyramid_features['P{}'.format(i)]\n",
    "                _cls_head_logits = self._classification_head(feature)\n",
    "                _reg_head_logits = self._regression_head(feature)\n",
    "                _reg_head_logits = \\\n",
    "                    Scale(init_value=1.0,\n",
    "                          name='P{}_reg_outputs'.format(i))(_reg_head_logits)\n",
    "\n",
    "                _classification_logits.append(_cls_head_logits[0][0])\n",
    "                _centerness_logits.append(_cls_head_logits[0][1])\n",
    "                _regression_logits.append(_reg_head_logits)\n",
    "\n",
    "            _image_input = self._backbone.input\n",
    "            outputs = [*_classification_logits,\n",
    "                       *_centerness_logits,\n",
    "                       *_regression_logits\n",
    "                       ]\n",
    "            self.model = tf.keras.Model(\n",
    "                inputs=[_image_input], outputs=outputs, name='FCOS')\n",
    "\n",
    "    def _build_datasets(self):\n",
    "        pprint('****Building Datasets')\n",
    "        with self.distribute_strategy.scope():\n",
    "            self.train_dataset, self.val_dataset, \\\n",
    "                num_train_images, num_val_images =  \\\n",
    "                self.dataset_fn(self.image_height,\n",
    "                                self.image_width,\n",
    "                                self.data_dir,\n",
    "                                self.batch_size)\n",
    "\n",
    "            self.training_steps = num_train_images // self.batch_size\n",
    "            self.val_steps = num_val_images // self.batch_size\n",
    "\n",
    "    def _build_optimizer(self):\n",
    "        pprint('****Setting Up Optimizer')\n",
    "        self.optimizer = tf.keras.optimizers.Adam(lr=self.learning_rate,\n",
    "                                                  clipnorm=0.0001)\n",
    "\n",
    "    def _initialize_metrics(self):\n",
    "        with self.distribute_strategy.scope():\n",
    "            pass\n",
    "\n",
    "    def restore_checkpoint(self, checkpoint_path):\n",
    "        self.checkpoint.restore(checkpoint_path)\n",
    "\n",
    "    def _create_checkpoint_manager(self):\n",
    "        with self.distribute_strategy.scope():\n",
    "            self.checkpoint = tf.train.Checkpoint(model=self.model,\n",
    "                                                  optimizer=self.optimizer)\n",
    "            if self.restore_parameters:\n",
    "                pprint('****Restoring Parameters')\n",
    "                pprint('****Restored Parameters')\n",
    "                self.restore_status = self.checkpoint.restore(\n",
    "                    self.latest_checkpoint)\n",
    "\n",
    "    def _create_summary_writer(self):\n",
    "        self.summary_writer = tf.summary.create_file_writer(\n",
    "            logdir=self.tensorboard_log_dir)\n",
    "\n",
    "    def _write_summaries(self, metrics):\n",
    "        pprint('****Writing Summaries')\n",
    "\n",
    "    def write_checkpoint(self):\n",
    "        with self.distribute_strategy.scope():\n",
    "            self.checkpoint.save(os.path.join(self.model_dir,\n",
    "                                              self.checkpoint_prefix))\n",
    "\n",
    "    def _update_metrics(self, metrics):\n",
    "        pass\n",
    "\n",
    "    def _reset_metrics(self):\n",
    "        for metric in self.metrics:\n",
    "            metric.reset_states()\n",
    "\n",
    "    def _log_metrics(self):\n",
    "        metrics_dict = {\n",
    "            'epoch': self.epoch,\n",
    "            'batch': self.iterations,\n",
    "        }\n",
    "        for metric in self.metrics:\n",
    "            metrics_dict.update({metric.name: np.round(metric.result(), 3)})\n",
    "        pprint(metrics_dict)\n",
    "\n",
    "    def _compute_loss(self, targets, cls_outputs,\n",
    "                      ctr_outputs, reg_outputs):\n",
    "        pass\n",
    "\n",
    "    def train(self):\n",
    "        # TODO\n",
    "        #   a) compute centers\n",
    "        #   b) Run custom training loop\n",
    "        #   c) Calculate loss separately for each feature level\n",
    "        assert self.mode == 'train', 'Cannot train in inference mode'\n",
    "        pprint('****Starting Training Loop')\n",
    "\n",
    "        @tf.function\n",
    "        def _train_step(images, targets):\n",
    "            with tf.GradientTape() as tape:\n",
    "                cls_outputs, ctr_outputs, reg_outputs = self.model(\n",
    "                    images, training=True)\n",
    "                cls_loss, ctr_loss, reg_losss = \\\n",
    "                    self._compute_loss(targets, cls_outputs,\n",
    "                                       ctr_outputs, reg_outputs)\n",
    "                loss = cls_loss + ctr_loss + reg_losss\n",
    "            gradients =  \\\n",
    "                tape.gradient(loss, self.model.trainable_variables)\n",
    "            self.optimizer.apply_gradients(zip(gradients,\n",
    "                                               self.model.trainable_variables))\n",
    "\n",
    "        @tf.function\n",
    "        def _distributed_train_step(images, targets):\n",
    "            per_replica_metrics = \\\n",
    "                self.distribute_strategy.experimental_run_v2(fn=_train_step,\n",
    "                                                             args=(images,\n",
    "                                                                   targets))\n",
    "            reduced_metrics = \\\n",
    "                self.distribute_strategy.reduce(tf.distribute.ReduceOp.SUM,\n",
    "                                                per_replica_metrics, axis=0)\n",
    "            return reduced_metrics\n",
    "\n",
    "        @tf.function\n",
    "        def _train():\n",
    "            self.epoch = 0\n",
    "            for _ in range(self.epochs):\n",
    "                self.iterations = 0\n",
    "                for images, targets in self.dataset:\n",
    "                    metrics = _distributed_train_step(images, targets)\n",
    "                    self.update_metrics(metrics)\n",
    "                    self.log_metrics()\n",
    "                    self.iterations += 1\n",
    "                self.write_summaries(metrics)\n",
    "                self.reset_metrics()\n",
    "                self.write_checkpoint()\n",
    "                self.epoch += 1\n",
    "        return _train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "data_dir = os.environ['HOME'] + '/datasets/BDD100k'\n",
    "\n",
    "config = {\n",
    "    'mode': 'train',\n",
    "    'distribute_strategy': strategy,\n",
    "    'image_height': 720,\n",
    "    'image_width': 1280,\n",
    "    'num_classes': 10,\n",
    "    'dataset_fn': dataset_fn,\n",
    "    'data_dir': data_dir,\n",
    "    'batch_size': 8,\n",
    "    'epochs': 50,\n",
    "    'learning_rate': 1e-4,\n",
    "    'model_dir': './model_files',\n",
    "    'tensorboard_log_dir': './logs',\n",
    "    'restore_parameters': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'****Initializing FCOS with the following config'\n",
      "{'batch_size': 8,\n",
      " 'data_dir': '/home/antpc/datasets/BDD100k',\n",
      " 'dataset_fn': <function dataset_fn at 0x7ff2acfb40d0>,\n",
      " 'distribute_strategy': <tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7ff1fc2fa7b8>,\n",
      " 'epochs': 50,\n",
      " 'image_height': 720,\n",
      " 'image_width': 1280,\n",
      " 'learning_rate': 0.0001,\n",
      " 'mode': 'train',\n",
      " 'model_dir': './model_files',\n",
      " 'num_classes': 10,\n",
      " 'restore_parameters': False,\n",
      " 'tensorboard_log_dir': './logs'}\n",
      "'****Building FPN'\n",
      "'****Building FCOS'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'FCOS' object has no attribute '_classification_logits'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-d60b5d2111d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfcos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFCOS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-90846601e9c9>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_fpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-90846601e9c9>\u001b[0m in \u001b[0;36m_build_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    137\u001b[0m                           name='P{}_reg_outputs'.format(i))(_reg_head_logits)\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_classification_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_cls_head_logits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_centerness_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_cls_head_logits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_regression_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_reg_head_logits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FCOS' object has no attribute '_classification_logits'"
     ]
    }
   ],
   "source": [
    "fcos = FCOS(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'classification_head_5/Identity:0' shape=(None, None, 10) dtype=float32>,\n",
       " <tf.Tensor 'classification_head_5/Identity_1:0' shape=(None, None, 1) dtype=float32>,\n",
       " <tf.Tensor 'P3_reg_outputs_1/Identity:0' shape=(None, None, 4) dtype=float32>,\n",
       " <tf.Tensor 'classification_head_6/Identity:0' shape=(None, None, 10) dtype=float32>,\n",
       " <tf.Tensor 'classification_head_6/Identity_1:0' shape=(None, None, 1) dtype=float32>,\n",
       " <tf.Tensor 'P4_reg_outputs_1/Identity:0' shape=(None, None, 4) dtype=float32>,\n",
       " <tf.Tensor 'classification_head_7/Identity:0' shape=(None, None, 10) dtype=float32>,\n",
       " <tf.Tensor 'classification_head_7/Identity_1:0' shape=(None, None, 1) dtype=float32>,\n",
       " <tf.Tensor 'P5_reg_outputs_1/Identity:0' shape=(None, None, 4) dtype=float32>,\n",
       " <tf.Tensor 'classification_head_8/Identity:0' shape=(None, None, 10) dtype=float32>,\n",
       " <tf.Tensor 'classification_head_8/Identity_1:0' shape=(None, None, 1) dtype=float32>,\n",
       " <tf.Tensor 'P6_reg_outputs_1/Identity:0' shape=(None, None, 4) dtype=float32>,\n",
       " <tf.Tensor 'classification_head_9/Identity:0' shape=(None, None, 10) dtype=float32>,\n",
       " <tf.Tensor 'classification_head_9/Identity_1:0' shape=(None, None, 1) dtype=float32>,\n",
       " <tf.Tensor 'P7_reg_outputs_1/Identity:0' shape=(None, None, 4) dtype=float32>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcos.model.outputsP3_reg_outputs_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
