{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.initializers import RandomNormal, Constant\n",
    "from tensorflow.keras.layers import (Input,\n",
    "                                     Conv2D, \n",
    "                                     Concatenate,\n",
    "                                     BatchNormalization,\n",
    "                                     Lambda,\n",
    "                                     ReLU,\n",
    "                                     Reshape,\n",
    "                                     Add)\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras import backend as K\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print('TensorFlow:', tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(input_tensor=None,\n",
    "               filters=None,\n",
    "               kernel_size=None,\n",
    "               strides=1,\n",
    "               padding='same',\n",
    "               kernel_init='he_normal',\n",
    "               bias_init='zeros',\n",
    "               bn_act=True,\n",
    "               name_prefix=None):\n",
    "    \n",
    "    _x = Conv2D(filters=filters, kernel_size=kernel_size,\n",
    "                padding=padding, strides=strides,\n",
    "                kernel_initializer=kernel_init,\n",
    "                bias_initializer=bias_init,\n",
    "                name='{}_conv_{}x{}'.format(name_prefix,\n",
    "                                            kernel_size,\n",
    "                                            kernel_size))(input_tensor)\n",
    "    if bn_act:\n",
    "        _x = BatchNormalization(\n",
    "            name='{}_bn'.format(name_prefix))(_x)\n",
    "        _x = ReLU(name='{}_relu'.format(name_prefix))(_x)\n",
    "    return _x\n",
    "\n",
    "\n",
    "def upsample_like(input_tensor, target_tensor, name=None):\n",
    "    _, fh, fw, _ = target_tensor.shape\n",
    "    _upsampled_tensor = tf.image.resize(input_tensor,\n",
    "                                        size=[fh, fw],\n",
    "                                        method='nearest', \n",
    "                                        name=name)\n",
    "    return _upsampled_tensor\n",
    "\n",
    "\n",
    "\n",
    "class Scale(tf.keras.layers.Layer):\n",
    "    def __init__(self, init_value=1.0, **kwargs):\n",
    "        super(Scale, self).__init__(**kwargs)\n",
    "        self.init_value = init_value\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.scale = \\\n",
    "            self.add_weight(name='scale',\n",
    "                            shape=[1],\n",
    "                            dtype=K.floatx(),\n",
    "                            trainable=True,\n",
    "                            initializer=Constant(value=self.init_value))\n",
    "\n",
    "    def call(self, x):\n",
    "        scaled_inputs = tf.multiply(self.scale, x)\n",
    "        return tf.exp(scaled_inputs)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Scale, self).get_config()\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCOS:\n",
    "    def __init__(self, config):\n",
    "        self._validate_config(config)\n",
    "        for attr in config:\n",
    "            setattr(self, attr, config[attr])\n",
    "        self._build_fpn()\n",
    "        self._build_model()\n",
    "        self._build_datasets()\n",
    "        self._build_optimizer()\n",
    "        self._build_callbacks()\n",
    "\n",
    "    def _validate_config(self, config):\n",
    "        attr_list = [\n",
    "            'mode',\n",
    "            'distribute_strategy',\n",
    "            'image_height',\n",
    "            'image_width',\n",
    "            'num_classes',\n",
    "            'data_dir',\n",
    "            'dataset_fn',\n",
    "            'batch_size',\n",
    "            'epochs',\n",
    "            'learning_rate',\n",
    "            'model_dir',\n",
    "            'tensorboard_log_dir'\n",
    "        ]\n",
    "        for attr in attr_list:\n",
    "            assert attr in config, 'Missing {} in config'.format(attr)\n",
    "        pprint('****Initializing FCOS with the following config')\n",
    "        pprint(config)\n",
    "\n",
    "    def _build_fpn(self):\n",
    "        '''\n",
    "            From the FPN paper, \"To start the iteration, we simply attach a\n",
    "            1×1 convolutional layer on C5 to produce the coarsest resolution\n",
    "            map. Finally, we append a 3×3 convolution on each merged map to\n",
    "            generate the final feature map, which is to reduce the aliasing\n",
    "            effect of upsampling. This final set of feature maps is called\n",
    "            {P2, P3, P4, P5}, corresponding to {C2, C3, C4, C5} that are\n",
    "            respectively of the same spatial sizes\".\n",
    "            From the FCOS paper, \"P6 and P7 are produced by applying one\n",
    "            convolutional layer with the stride being 2 on P5 and P6,\n",
    "            respectively\".\n",
    "        '''\n",
    "        with self.distribute_strategy.scope():\n",
    "            pprint('****Building FPN')\n",
    "            self._backbone = tf.keras.applications.ResNet50V2(\n",
    "                input_shape=[self.image_height, self.image_width, 3],\n",
    "                weights='imagenet',\n",
    "                include_top=False)\n",
    "            C5 = self._backbone.get_layer('post_relu').output\n",
    "            C4 = self._backbone.get_layer('conv4_block6_1_relu').output\n",
    "            C3 = self._backbone.get_layer('conv3_block4_1_relu').output\n",
    "\n",
    "            M5 = conv_block(C5, 256, 1, bn_act=False, name_prefix='C5')\n",
    "            P5 = conv_block(M5, 256, 3, bn_act=False, name_prefix='P5')\n",
    "            M5_upsampled = upsample_like(M5, C4, name='M5_upsampled')\n",
    "\n",
    "            M4 = conv_block(C4, 256, 1, bn_act=False, name_prefix='C4')\n",
    "            M4 = tf.keras.layers.Add(name='M4_M5_add')([M4, M5_upsampled])\n",
    "            P4 = conv_block(M4, 256, 3, bn_act=False, name_prefix='P4')\n",
    "            M4_upsampled = upsample_like(M4, C3, name='M4_upsampled')\n",
    "\n",
    "            M3 = conv_block(C3, 256, 1, bn_act=False, name_prefix='C3')\n",
    "            P3 = Add(name='M3_M4_add')([M3, M4_upsampled])\n",
    "            P3 = conv_block(P3, 256, 3, bn_act=False, name_prefix='P3')\n",
    "\n",
    "            P6 = conv_block(P5, 256, 3, 2, bn_act=False, name_prefix='P6')\n",
    "            P6_relu = ReLU(name='P6_relu')(P6)\n",
    "            P7 = conv_block(P6_relu, 256, 3, 2, bn_act=False, name_prefix='P7')\n",
    "\n",
    "            self._pyramid_features = {\n",
    "                'P3': P3,\n",
    "                'P4': P4,\n",
    "                'P5': P5,\n",
    "                'P6': P6,\n",
    "                'P7': P7\n",
    "            }\n",
    "\n",
    "    def _get_classification_head(self, p=0.01):\n",
    "        kernel_init = RandomNormal(0.0, 0.01)\n",
    "        bias_init = Constant(-np.log((1 - p) / p))\n",
    "\n",
    "        input_layer = Input(shape=[None, None, 256])\n",
    "        x = input_layer\n",
    "\n",
    "        for i in range(4):\n",
    "            x = conv_block(x, 256, 3, kernel_init=kernel_init,\n",
    "                           bn_act=False, name_prefix='c_head_{}'.format(i))\n",
    "        classification_logits = conv_block(x, self.num_classes,\n",
    "                                           3, kernel_init=kernel_init,\n",
    "                                           bias_init=bias_init, bn_act=False,\n",
    "                                           name_prefix='cls_logits')\n",
    "        centerness_logits = conv_block(x, 1, 3,\n",
    "                                       kernel_init=kernel_init, bn_act=False,\n",
    "                                       name_prefix='ctr_logits')\n",
    "        classification_logits = Reshape(\n",
    "            target_shape=[-1, self.num_classes])(classification_logits)\n",
    "        centerness_logits = Reshape(target_shape=[-1, 1])(centerness_logits)\n",
    "\n",
    "        outputs = [classification_logits, centerness_logits]\n",
    "        return tf.keras.Model(inputs=[input_layer],\n",
    "                              outputs=[outputs],\n",
    "                              name='classification_head')\n",
    "\n",
    "    def _get_regression_head(self):\n",
    "        kernel_init = RandomNormal(0.0, 0.01)\n",
    "        input_layer = Input(shape=[None, None, 256])\n",
    "        x = input_layer\n",
    "\n",
    "        for i in range(4):\n",
    "            x = conv_block(x, 256, 3, kernel_init=kernel_init,\n",
    "                           bn_act=False, name_prefix='r_head_{}'.format(i))\n",
    "        regression_logits = conv_block(x, 4, 3, kernel_init=kernel_init,\n",
    "                                       bn_act=False, name_prefix='reg_logits')\n",
    "        regression_logits = Reshape(target_shape=[-1, 4])(regression_logits)\n",
    "        return tf.keras.Model(inputs=[input_layer],\n",
    "                              outputs=[regression_logits],\n",
    "                              name='regression_head')\n",
    "\n",
    "    def _build_model(self):\n",
    "        with self.distribute_strategy.scope():\n",
    "            pprint('****Building FCOS')\n",
    "            self._classification_head = self._get_classification_head()\n",
    "            self._regression_head = self._get_regression_head()\n",
    "\n",
    "            self._classification_logits = []\n",
    "            self._centerness_logits = []\n",
    "            self._regression_logits = []\n",
    "\n",
    "            for i in range(3, 8):\n",
    "                feature = self._pyramid_features['P{}'.format(i)]\n",
    "                _cls_head_logits = self._classification_head(feature)\n",
    "                _reg_head_logits = self._regression_head(feature)\n",
    "                _reg_head_logits = \\\n",
    "                    Scale(init_value=1.0,\n",
    "                          name='P{}_reg_outputs'.format(i))(_reg_head_logits)\n",
    "\n",
    "                self._classification_logits.append(_cls_head_logits[0][0])\n",
    "                self._centerness_logits.append(_cls_head_logits[0][1])\n",
    "                self._regression_logits.append(_reg_head_logits)\n",
    "\n",
    "            self._classification_logits = Concatenate(\n",
    "                axis=1,\n",
    "                name='classification_outputs')(self._classification_logits)\n",
    "            self._centerness_logits = Concatenate(\n",
    "                axis=1, name='centerness_outputs')(self._centerness_logits)\n",
    "            self._regression_logits = Concatenate(\n",
    "                axis=1, name='regression_outputs')(self._regression_logits)\n",
    "\n",
    "            _image_input = self._backbone.input\n",
    "            outputs = [self._classification_logits,\n",
    "                       self._centerness_logits,\n",
    "                       self._regression_logits]\n",
    "            self.model = tf.keras.Model(\n",
    "                inputs=[_image_input], outputs=outputs, name='FCOS')\n",
    "            self.model.build([self.image_height, self.image_width, 3])\n",
    "\n",
    "    def _build_datasets(self):\n",
    "        pprint('****Building Datasets')\n",
    "        with self.distribute_strategy.scope():\n",
    "            self.train_dataset, self.val_dataset, \\\n",
    "                num_train_images, num_val_images =  \\\n",
    "                self.dataset_fn(self.image_height,\n",
    "                                self.image_width,\n",
    "                                self.data_dir,\n",
    "                                self.batch_size)\n",
    "\n",
    "            self.train_steps = num_train_images // self.batch_size\n",
    "            self.val_steps = num_val_images // self.batch_size\n",
    "\n",
    "    def _build_callbacks(self):\n",
    "        pprint('****Setting Up Callbacks')\n",
    "        self.callbacks = [\n",
    "            TensorBoard(log_dir=self.tensorboard_log_dir),\n",
    "            ModelCheckpoint(filepath=self.model_dir + '/ckpt-{epoch:02d}',\n",
    "                            monitor='val_loss',\n",
    "                            save_weights_only=True,\n",
    "                            save_best_only=True)\n",
    "        ]\n",
    "\n",
    "    def _build_optimizer(self):\n",
    "        pprint('****Setting Up Optimizer')\n",
    "        self.optimizer = tf.keras.optimizers.Adam(lr=self.learning_rate)\n",
    "\n",
    "    def _classification_loss(self, alpha=0.25, gamma=2):\n",
    "        # TODO\n",
    "        #   a) Double check if tf.keras.Model.fit is handling\n",
    "        #      loss scaling for distributed training if not\n",
    "        #      use tf.nn.compute_average_loss fn\n",
    "        def focal_loss(y_true, y_pred):\n",
    "            fg_mask = tf.cast(y_true != 0, dtype=tf.float32)\n",
    "            y_true = tf.one_hot(\n",
    "                tf.cast(y_true, dtype=tf.int32), depth=self.num_classes + 1)\n",
    "            y_true = y_true[:, :, 1:]\n",
    "            y_pred_ = tf.sigmoid(y_pred)\n",
    "\n",
    "            at = alpha * y_true + (1 - y_true) * (1 - alpha)\n",
    "            pt = y_true * y_pred_ + (1 - y_true) * (1 - y_pred_)\n",
    "            f_loss = at * \\\n",
    "                tf.pow(1 - pt, gamma) * \\\n",
    "                tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    labels=y_true, logits=y_pred)\n",
    "            f_loss = tf.reduce_mean(f_loss, axis=2)\n",
    "            f_loss = f_loss * fg_mask\n",
    "            f_loss = tf.reduce_sum(f_loss, axis=1, keepdims=True)\n",
    "            normalizer_value = tf.reduce_sum(fg_mask, axis=1, keepdims=True)\n",
    "            f_loss = f_loss / normalizer_value\n",
    "            return f_loss\n",
    "        return focal_loss\n",
    "\n",
    "    def _centerness_loss(self, labels, logits):\n",
    "        # TODO\n",
    "        #   a) Double check if tf.keras.Model.fit is handling\n",
    "        #      loss scaling for distributed training if not\n",
    "        #      use tf.nn.compute_average_loss fn\n",
    "        fg_mask = tf.cast(labels != 0, dtype=tf.float32)\n",
    "        bce_loss = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            labels=labels, logits=logits)\n",
    "        bce_loss = bce_loss * fg_mask\n",
    "        bce_loss = tf.reduce_sum(bce_loss, axis=1)\n",
    "        normalizer_value = tf.reduce_sum(fg_mask, axis=1)\n",
    "        bce_loss = bce_loss / normalizer_value\n",
    "        return bce_loss\n",
    "\n",
    "    def _regression_loss(self, labels, logits):\n",
    "        # TODO\n",
    "        #   a) IOU loss\n",
    "        #   b) mask negative locations\n",
    "        #   c) normalize loss value\n",
    "        pass\n",
    "\n",
    "    def train(self):\n",
    "        loss_dict = {\n",
    "            'classification_outputs': self._classification_loss(alpha=0.25,\n",
    "                                                                gamma=2),\n",
    "            'centerness_outputs': self._centerness_loss,\n",
    "            'regression_outputs': self._regression_loss\n",
    "        }\n",
    "        with self.distribute_strategy.scope():\n",
    "            self.model.compile(optimizer=self.optimizer,\n",
    "                               loss=loss_dict)\n",
    "            self.model.fit(self.train_dataset,\n",
    "                           epochs=self.epochs,\n",
    "                           steps_per_epoch=self.training_steps,\n",
    "                           validation_data=self.val_dataset,\n",
    "                           validation_steps=self.val_steps,\n",
    "                           validation_freq=2,\n",
    "                           callbacks=self.callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def flip_data(image, boxes, w):\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        image = tf.image.flip_left_right(image)\n",
    "        boxes = tf.stack([\n",
    "            w - boxes[:, 2],\n",
    "            boxes[:, 1],\n",
    "            w - boxes[:, 0],\n",
    "            boxes[:, 3]\n",
    "        ], axis=-1)\n",
    "    return image, boxes\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def random_jitter(image):\n",
    "    # ToDo\n",
    "    pass\n",
    "\n",
    "\n",
    "def compute_area(boxes):\n",
    "    h_ = boxes[:, 2] - boxes[:, 0]\n",
    "    w_ = boxes[:, 3] - boxes[:, 1]\n",
    "    return h_ * w_\n",
    "\n",
    "\n",
    "def compute_feature_sizes(H, W):\n",
    "    fm_sizes = []\n",
    "    for i in range(3, 8):\n",
    "        stride = 2.**i\n",
    "        fm_sizes.append([tf.math.ceil(H / stride),\n",
    "                         tf.math.ceil(W / stride), stride])\n",
    "    return fm_sizes\n",
    "\n",
    "\n",
    "def get_centers(fm_h, fm_w, stride=None):\n",
    "    rx = (tf.range(fm_w) + 0.5) * (stride)\n",
    "    ry = (tf.range(fm_h) + 0.5) * (stride)\n",
    "    sx, sy = tf.meshgrid(rx, ry)\n",
    "    cxy = tf.stack([sx, sy], axis=-1)\n",
    "    return cxy\n",
    "\n",
    "\n",
    "def get_all_centers(H, W):\n",
    "    centers_list = []\n",
    "    feature_sizes = compute_feature_sizes(H, W)\n",
    "    for fm_h, fm_w, stride in feature_sizes:\n",
    "        cyx = get_centers(fm_h, fm_w, stride)\n",
    "        cyx = tf.reshape(cyx, shape=[-1, 2])\n",
    "        centers_list.append(cyx)\n",
    "    return centers_list\n",
    "\n",
    "\n",
    "@tf.function(input_signature=[\n",
    "    tf.TensorSpec(shape=[None, 2], dtype=tf.float32),\n",
    "    tf.TensorSpec(shape=[None, 5], dtype=tf.float32),\n",
    "    tf.TensorSpec(shape=[], dtype=tf.float32),\n",
    "    tf.TensorSpec(shape=[], dtype=tf.float32)\n",
    "])\n",
    "def compute_targets_(centers, labels, low, high):\n",
    "    '''\n",
    "        From the FCOS paper, \"Specifically, location (x, y) is\n",
    "        considered as a positive sample if it falls into any\n",
    "        ground-truth box and the class label c* of the location is\n",
    "        the class label of the ground-truth box. Otherwise it is a\n",
    "        negative sample and class* = 0 (background class)\n",
    "        Besides the label for classification, we also have a 4D\n",
    "        real vector t* = (l*, t*, r*, b*) being the regression\n",
    "        targets for the location. Here l*, t*, r* and b* are the\n",
    "        distances from the location to the four sides of the bounding\n",
    "        box ...If a location falls into multiple bounding boxes, it is\n",
    "        considered as an ambiguous sample. We simply choose the\n",
    "        bounding box with minimal area as its regression target.\n",
    "        ...we firstly compute the regressiontargets l*, t*, r* and b*\n",
    "        for each location on all feature levels. Next, if a location\n",
    "        satisfies max(l*, t*, r*, b*) > mi or max(l*, t*, r*, b*) < mi−1,\n",
    "        it is set as a negative sample and is thus not required to\n",
    "        regress a bounding box anymore. Here mi is the maximum distance\n",
    "        that feature level i needs to regress. In this work, m2, m3, m4,\n",
    "        m5, m6 and m7 are set as 0, 64, 128, 256, 512 and ∞, respectively\"\n",
    "        Args:\n",
    "            centers (M, 2): Centers for the current feature level\n",
    "            labels (N, 5):  All labels for the current image\n",
    "            low: Lower limit for ltrb value for the current feature level\n",
    "            high: Upper limit for ltrb value for the current feature level\n",
    "    '''\n",
    "    boxes_ = labels[:, :4]\n",
    "    class_ids_ = labels[:, 4]\n",
    "\n",
    "    # Sorted the boxes by area in ascending order so that\n",
    "    # we pick the smallest box when computing ltbr values\n",
    "    areas = compute_area(boxes_)\n",
    "    sorted_indices = tf.argsort(areas)\n",
    "    boxes = tf.gather(boxes_, indices=sorted_indices)\n",
    "    class_ids = tf.gather(class_ids_, indices=sorted_indices)\n",
    "\n",
    "    xy_min_ = boxes[:, :2]\n",
    "    xy_max_ = boxes[:, 2:]\n",
    "    lt_ = centers[:, None] - xy_min_\n",
    "    rb_ = xy_max_ - centers[:, None]\n",
    "    ltrb_ = tf.concat([lt_, rb_], axis=2)  # (M, N, 4)\n",
    "\n",
    "    # check if max(lbtr) lies in the valid_range for this\n",
    "    # feature level\n",
    "    max_ltrb_ = tf.reduce_max(ltrb_, axis=2)  # (M, N)\n",
    "    mask_ltrb_size = tf.logical_and(max_ltrb_ > low, max_ltrb_ < high)\n",
    "\n",
    "    mask_lt = tf.logical_and(ltrb_[:, :, 0] > 0, ltrb_[:, :, 1] > 0)\n",
    "    mask_rb = tf.logical_and(ltrb_[:, :, 2] > 0, ltrb_[:, :, 3] > 0)\n",
    "    mask = tf.logical_and(mask_lt, mask_rb)\n",
    "    mask = tf.logical_and(mask, mask_ltrb_size)  # (M, N)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    fg_mask = tf.reduce_sum(mask, axis=1) != 0  # (M,)\n",
    "    fg_mask = tf.cast(fg_mask, dtype=tf.float32)\n",
    "    fg_mask = tf.tile(fg_mask[:, None], multiples=[1, 4])\n",
    "\n",
    "    valid_indices = tf.argmax(mask, axis=1)  # (M, )\n",
    "    matched_boxes = tf.gather(boxes, valid_indices)\n",
    "    matched_class_ids = tf.gather(class_ids, valid_indices) + 1\n",
    "\n",
    "    x_min, y_min, x_max, y_max = tf.split(matched_boxes,\n",
    "                                          num_or_size_splits=4,\n",
    "                                          axis=1)\n",
    "    l = tf.abs(centers[:, 0] - x_min[:, 0])\n",
    "    t = tf.abs(centers[:, 1] - y_min[:, 0])\n",
    "    r = tf.abs(x_max[:, 0] - centers[:, 0])\n",
    "    b = tf.abs(y_max[:, 0] - centers[:, 1])\n",
    "    lr = tf.stack([l, r], axis=1)\n",
    "    tb = tf.stack([t, b], axis=1)\n",
    "\n",
    "    min_lr = tf.reduce_min(lr, axis=1)\n",
    "    max_lr = tf.reduce_max(lr, axis=1)\n",
    "    min_tb = tf.reduce_min(tb, axis=1)\n",
    "    max_tb = tf.reduce_max(tb, axis=1)\n",
    "\n",
    "    classification_target = matched_class_ids * fg_mask[:, 0]\n",
    "    centerness_target = tf.sqrt(\n",
    "        (min_lr / max_lr) * (min_tb / max_tb)) * fg_mask[:, 0]\n",
    "    regression_target = tf.stack([l, t, r, b], axis=1) * fg_mask\n",
    "\n",
    "    return classification_target, centerness_target, regression_target\n",
    "\n",
    "\n",
    "def compute_targets(H, W, labels):\n",
    "    centers_list = get_all_centers(H, W)\n",
    "    m = [\n",
    "        [0.0, 64.0],\n",
    "        [64.0, 128.0],\n",
    "        [128.0, 256.0],\n",
    "        [256.0, 512.0],\n",
    "        [512.0, 1e8]]\n",
    "    classification_target = []\n",
    "    centerness_target = []\n",
    "    regression_target = []\n",
    "    for i in range(5):\n",
    "        centers = centers_list[i]\n",
    "        low, high = m[i]\n",
    "        cls_target, \\\n",
    "            ctr_target, \\\n",
    "            reg_target = compute_targets_(centers, labels, low, high)\n",
    "\n",
    "        classification_target.append(cls_target)\n",
    "        centerness_target.append(ctr_target)\n",
    "        regression_target.append(reg_target)\n",
    "\n",
    "    classification_target = tf.concat(classification_target, axis=0)\n",
    "    centerness_target = tf.concat(centerness_target, axis=0)\n",
    "    centerness_target = tf.expand_dims(centerness_target, axis=-1)\n",
    "    regression_target = tf.concat(regression_target, axis=0)\n",
    "    return classification_target, centerness_target, regression_target\n",
    "\n",
    "\n",
    "feature_description = {\n",
    "    'image': tf.io.FixedLenFeature([], tf.string),\n",
    "    'xmins': tf.io.VarLenFeature(tf.float32),\n",
    "    'ymins': tf.io.VarLenFeature(tf.float32),\n",
    "    'xmaxs': tf.io.VarLenFeature(tf.float32),\n",
    "    'ymaxs': tf.io.VarLenFeature(tf.float32),\n",
    "    'labels': tf.io.VarLenFeature(tf.float32)\n",
    "}\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def parse_example(example_proto):\n",
    "    parsed_example = tf.io.parse_single_example(\n",
    "        example_proto, feature_description)\n",
    "    image = tf.image.decode_jpeg(parsed_example['image'], channels=3)\n",
    "    bboxes = tf.stack([\n",
    "        tf.sparse.to_dense(parsed_example['xmins']),\n",
    "        tf.sparse.to_dense(parsed_example['ymins']),\n",
    "        tf.sparse.to_dense(parsed_example['xmaxs']),\n",
    "        tf.sparse.to_dense(parsed_example['ymaxs'])\n",
    "    ], axis=-1)\n",
    "    class_ids = tf.reshape(tf.sparse.to_dense(\n",
    "        parsed_example['labels']), [-1, 1])\n",
    "    return image, bboxes, class_ids\n",
    "\n",
    "\n",
    "\n",
    "def load_data(h, w):\n",
    "    @tf.function\n",
    "    def load_data_(example_proto):\n",
    "        image, boxes_, class_ids = parse_example(example_proto)\n",
    "        image.set_shape([None, None, 3])\n",
    "        image = tf.image.resize(image, size=[h, w])\n",
    "        boxes = tf.stack([\n",
    "            tf.clip_by_value(boxes_[:, 0] * w, 0, w),\n",
    "            tf.clip_by_value(boxes_[:, 1] * h, 0, h),\n",
    "            tf.clip_by_value(boxes_[:, 2] * w, 0, w),\n",
    "            tf.clip_by_value(boxes_[:, 3] * h, 0, h)\n",
    "        ], axis=-1)\n",
    "        image, boxes = flip_data(image, boxes, w)\n",
    "        label = tf.concat([boxes, class_ids], axis=-1)\n",
    "        classification_target, centerness_target, regression_target = \\\n",
    "            compute_targets(h, w, label)\n",
    "        return image, \\\n",
    "            (classification_target, centerness_target, regression_target)\n",
    "    return load_data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(H, W, tf_records_pattern, batch_size):\n",
    "    autotune = tf.data.experimental.AUTOTUNE\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_deterministic = False\n",
    "    train_files = tf.data.Dataset.list_files(tf_records_pattern)\n",
    "    dataset = train_files.interleave(tf.data.TFRecordDataset,\n",
    "                                     cycle_length=16,\n",
    "                                     block_length=16,\n",
    "                                     num_parallel_calls=autotune)\n",
    "    dataset = dataset.map(\n",
    "        load_data(H, W), num_parallel_calls=autotune)\n",
    "    dataset = dataset.shuffle(512)\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True).repeat()\n",
    "    dataset = dataset.prefetch(autotune)\n",
    "    dataset = dataset.with_options(options)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def dataset_fn(H, W, data_dir, batch_size):\n",
    "    train_tf_records_pattern = data_dir + '/train*'\n",
    "    val_tf_records_pattern = data_dir + '/val*'\n",
    "    train_dataset = \\\n",
    "        create_dataset(H, W, train_tf_records_pattern, batch_size)\n",
    "    val_dataset = \\\n",
    "        create_dataset(H, W, val_tf_records_pattern, batch_size)\n",
    "    num_train_images = 70000\n",
    "    num_val_images = 10000\n",
    "    return train_dataset, val_dataset, num_train_images, num_val_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'mode': 'train',\n",
    "    'distribute_strategy': tf.distribute.MirroredStrategy(),\n",
    "    'image_height': 720,\n",
    "    'image_width': 1280,\n",
    "    'num_classes': 10,\n",
    "    'dataset_fn': dataset_fn,\n",
    "    'data_dir': '../tfrecords',\n",
    "    'batch_size': 4,\n",
    "    'epochs': 250,\n",
    "    'learning_rate': 1e-4,\n",
    "    'model_dir': 'model_files',\n",
    "    'tensorboard_log_dir': 'logs'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'****Initializing FCOS with the following config'\n",
      "{'batch_size': 4,\n",
      " 'data_dir': '../tfrecords',\n",
      " 'dataset_fn': <function dataset_fn at 0x7fdbf611ad90>,\n",
      " 'distribute_strategy': <tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7fdbfea6f358>,\n",
      " 'epochs': 250,\n",
      " 'image_height': 720,\n",
      " 'image_width': 1280,\n",
      " 'learning_rate': 0.0001,\n",
      " 'mode': 'train',\n",
      " 'model_dir': 'model_files',\n",
      " 'num_classes': 10,\n",
      " 'tensorboard_log_dir': 'logs'}\n",
      "'****Building FPN'\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "'****Building FCOS'\n",
      "'****Building Datasets'\n",
      "'****Setting Up Optimizer'\n",
      "'****Setting Up Callbacks'\n"
     ]
    }
   ],
   "source": [
    "fcos = FCOS(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad634167820b4c52ba372428a8bec089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dummy_tensor = tf.random.normal(shape=[4, 720, 1280, 3])\n",
    "for i in tqdm(range(200)):\n",
    "    dummy_output = fcos.model(dummy_tensor, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrecords_base_dir = '../tfrecords'\n",
    "BATCH_SIZE = 2\n",
    "H, W = tf.constant([720., 1280.])\n",
    "autotune = tf.data.experimental.AUTOTUNE\n",
    "class_map = {value: idx for idx, value in enumerate(['bus',\n",
    "                                                     'traffic light',\n",
    "                                                     'traffic sign',\n",
    "                                                     'person',\n",
    "                                                     'bike',\n",
    "                                                     'truck',\n",
    "                                                     'motor',\n",
    "                                                     'car',\n",
    "                                                     'train',\n",
    "                                                     'rider'])}\n",
    "num_classes = len(class_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(2, 720, 1280, 3), dtype=tf.float32, name=None),\n",
       " (TensorSpec(shape=(2, 19220), dtype=tf.float32, name=None),\n",
       "  TensorSpec(shape=(2, 19220, 1), dtype=tf.float32, name=None),\n",
       "  TensorSpec(shape=(2, 19220, 4), dtype=tf.float32, name=None)))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "options = tf.data.Options()\n",
    "options.experimental_deterministic = False\n",
    "train_files = tf.data.Dataset.list_files('{}/train*'.format(tfrecords_base_dir))\n",
    "train_dataset = train_files.interleave(tf.data.TFRecordDataset,\n",
    "                                       cycle_length=16,\n",
    "                                       block_length=16,\n",
    "                                       num_parallel_calls=autotune)\n",
    "train_dataset = train_dataset.map(\n",
    "    load_data(H, W), num_parallel_calls=autotune)\n",
    "train_dataset = train_dataset.shuffle(512)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True).repeat()\n",
    "train_dataset = train_dataset.prefetch(autotune)\n",
    "train_dataset = train_dataset.with_options(options)\n",
    "tf.data.experimental.get_structure(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64ba26e56f5b45378bfa7084908fbb5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of positive centers: 820.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TensorShape([2, 720, 1280, 3]),\n",
       " TensorShape([2, 19220]),\n",
       " TensorShape([2, 19220, 1]),\n",
       " TensorShape([2, 19220, 4]))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _centerness_loss(labels, logits):\n",
    "    # TODO\n",
    "    #   a) mask negative locations\n",
    "    #   b) normalize loss value\n",
    "    fg_mask = tf.cast(labels != 0, dtype=tf.float32)\n",
    "    tf.print(tf.reduce_sum(fg_mask))\n",
    "    bce_loss = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "        labels=labels, logits=logits)\n",
    "    bce_loss = bce_loss * fg_mask\n",
    "    tf.print(bce_loss.shape)\n",
    "    bce_loss = tf.reduce_sum(bce_loss, axis=1)\n",
    "    tf.print(bce_loss.shape)\n",
    "    normalizer_value = tf.reduce_sum(fg_mask, axis=1)\n",
    "    tf.print(normalizer_value.shape)\n",
    "    bce_loss = bce_loss / normalizer_value\n",
    "    return bce_loss\n",
    "\n",
    "for batch in tqdm(train_dataset.take(1)):\n",
    "    image, (cls_target, ctr_target, reg_target) = batch\n",
    "    cls_target = tf.cast(cls_target, dtype=tf.int32)\n",
    "    positive_centers = tf.reduce_sum(tf.cast(cls_target != 0,\n",
    "                                             dtype=tf.float32))\n",
    "print('Number of positive centers: {}'.format(positive_centers.numpy()))\n",
    "cls_out, ctr_out, reg_out = fcos.model(image, training=False)\n",
    "image.shape, cls_target.shape, ctr_target.shape, reg_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820\n",
      "TensorShape([2, 19220, 1])\n",
      "TensorShape([2, 1])\n",
      "TensorShape([2, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=285911, shape=(2, 1), dtype=float32, numpy=\n",
       "array([[1.2419659],\n",
       "       [1.6207837]], dtype=float32)>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = _centerness_loss(ctr_target, ctr_out)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = tf.one_hot(cls_target[0][0], 11)[1:]\n",
    "y_pred = cls_out[0][0]\n",
    "y_pred_ = tf.sigmoid(y_pred)\n",
    "alpha = 0.25\n",
    "gamma = 2.0\n",
    "\n",
    "at = alpha * y_true + (1 - y_true) * (1 - alpha)\n",
    "pt = y_true * y_pred_ + (1 - y_true) * (1 - y_pred_)\n",
    "f_loss = at * \\\n",
    "    tf.pow(1 - pt, gamma) * \\\n",
    "    tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "        labels=y_true, logits=y_pred)\n",
    "f_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=256418, shape=(4,), dtype=float32, numpy=\n",
       "array([1.1283818e+00, 7.5377369e-07, 7.5377369e-07, 1.1283818e+00],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = tf.constant([1., 0., 0., 1.])\n",
    "y_pred = tf.constant([-4.59511985013459, -4.59511985013459, -4.59511985013459, -4.59511985013459])\n",
    "y_pred_ = tf.sigmoid(y_pred)\n",
    "alpha = 0.25\n",
    "gamma = 2.0\n",
    "\n",
    "at = alpha * y_true + (1 - y_true) * (1 - alpha)\n",
    "pt = y_true * y_pred_ + (1 - y_true) * (1 - y_pred_)\n",
    "f_loss = at * \\\n",
    "    tf.pow(1 - pt, gamma) * \\\n",
    "    tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "        labels=y_true, logits=y_pred)\n",
    "f_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=256355, shape=(10,), dtype=float32, numpy=\n",
       "array([0.01023436, 0.00952402, 0.01098686, 0.01150885, 0.00954002,\n",
       "       0.01119524, 0.01038414, 0.00972429, 0.00950115, 0.01010294],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
